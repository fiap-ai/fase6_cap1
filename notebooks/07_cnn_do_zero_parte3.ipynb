{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Treinamento de CNN do Zero - Parte 3: Avaliação e Comparação\n",
    "\n",
    "Este notebook implementa a terceira parte do treinamento de uma Rede Neural Convolucional (CNN) do zero para classificar imagens nas mesmas categorias que usamos nos modelos YOLO.\n",
    "\n",
    "Nesta terceira parte, vamos focar na avaliação do modelo treinado e na comparação com os modelos YOLO."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuração do Ambiente\n",
    "\n",
    "Primeiro, vamos importar as bibliotecas necessárias e configurar o ambiente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificar se o ambiente já foi configurado\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Se o ambiente ainda não foi configurado, execute o setup_env.sh\n",
    "if not os.path.exists('../yolov5'):\n",
    "    print(\"Configurando o ambiente com setup_env.sh...\")\n",
    "    !chmod +x ../setup_env.sh\n",
    "    !../setup_env.sh\n",
    "else:\n",
    "    print(\"Ambiente já configurado.\")\n",
    "\n",
    "# Importar bibliotecas\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import yaml\n",
    "import time\n",
    "import random\n",
    "from pathlib import Path\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Carregamento dos Dados e Modelo Treinado\n",
    "\n",
    "Vamos carregar os dados e o modelo treinado nas partes anteriores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificar se os arquivos necessários existem\n",
    "if not os.path.exists('../models/cnn/cnn_best.pt'):\n",
    "    print(\"❌ Arquivo '../models/cnn/cnn_best.pt' não encontrado. Execute a Parte 2 primeiro.\")\n",
    "else:\n",
    "    print(\"✅ Arquivo '../models/cnn/cnn_best.pt' encontrado.\")\n",
    "\n",
    "if not os.path.exists('../models/cnn/cnn_categories.txt'):\n",
    "    print(\"❌ Arquivo '../models/cnn/cnn_categories.txt' não encontrado. Execute a Parte 1 primeiro.\")\n",
    "    # Usar categorias padrão\n",
    "    categories = ['apple', 'banana']\n",
    "else:\n",
    "    print(\"✅ Arquivo '../models/cnn/cnn_categories.txt' encontrado.\")\n",
    "    # Carregar categorias\n",
    "    with open('../models/cnn/cnn_categories.txt', 'r') as f:\n",
    "        categories = [line.strip() for line in f.readlines()]\n",
    "    print(f\"Categorias: {categories}\")\n",
    "\n",
    "# Verificar se as métricas de treinamento foram salvas\n",
    "if not os.path.exists('../models/cnn/cnn_training_metrics.npy'):\n",
    "    print(\"❌ Arquivo '../models/cnn/cnn_training_metrics.npy' não encontrado. Execute a Parte 2 primeiro.\")\n",
    "    # Criar métricas vazias\n",
    "    training_metrics = {\n",
    "        'train_losses': [],\n",
    "        'train_accs': [],\n",
    "        'val_losses': [],\n",
    "        'val_accs': [],\n",
    "        'training_time': 0\n",
    "    }\n",
    "else:\n",
    "    print(\"✅ Arquivo '../models/cnn/cnn_training_metrics.npy' encontrado.\")\n",
    "    # Carregar métricas\n",
    "    training_metrics = np.load('../models/cnn/cnn_training_metrics.npy', allow_pickle=True).item()\n",
    "    print(f\"Tempo de treinamento: {training_metrics['training_time']:.2f} segundos\")\n",
    "\n",
    "# Definir diretório de teste\n",
    "test_dir = '../dataset/test/images'\n",
    "\n",
    "# Verificar se o diretório existe\n",
    "if not os.path.exists(test_dir):\n",
    "    print(f\"❌ Diretório não encontrado: {test_dir}\")\n",
    "else:\n",
    "    print(f\"✅ Diretório encontrado: {test_dir}\")\n",
    "    print(f\"   Número de imagens: {len([f for f in os.listdir(test_dir) if f.endswith(('.jpg', '.jpeg', '.png', '.avif'))])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Recriação do Dataset de Teste\n",
    "\n",
    "Vamos recriar o dataset de teste para avaliação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir transformações para as imagens\n",
    "test_transforms = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Classe personalizada para o dataset\n",
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, img_dir, categories, transform=None):\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.categories = categories\n",
    "        \n",
    "        # Listar todas as imagens\n",
    "        self.img_files = [f for f in os.listdir(img_dir) if f.endswith(('.jpg', '.jpeg', '.png', '.avif'))]\n",
    "        \n",
    "        # Determinar a classe de cada imagem com base no nome do arquivo\n",
    "        self.labels = []\n",
    "        for img_file in self.img_files:\n",
    "            # Assumindo que o nome do arquivo começa com o nome da categoria\n",
    "            # Por exemplo: categoria_a_001.jpg -> categoria_a\n",
    "            for i, category in enumerate(categories):\n",
    "                if category.lower() in img_file.lower():\n",
    "                    self.labels.append(i)\n",
    "                    break\n",
    "            else:\n",
    "                # Se não encontrar a categoria no nome do arquivo, usar a primeira categoria\n",
    "                self.labels.append(0)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.img_files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.img_files[idx])\n",
    "        image = cv2.imread(img_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, label\n",
    "\n",
    "# Criar dataset de teste\n",
    "test_dataset = CustomImageDataset(test_dir, categories, transform=test_transforms)\n",
    "\n",
    "# Criar dataloader\n",
    "batch_size = 16\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "# Verificar o dataset\n",
    "print(f\"Tamanho do dataset de teste: {len(test_dataset)}\")\n",
    "\n",
    "# Verificar a distribuição das classes\n",
    "test_labels = test_dataset.labels\n",
    "print(\"\\nDistribuição das classes no conjunto de teste:\")\n",
    "for i, category in enumerate(categories):\n",
    "    count = test_labels.count(i)\n",
    "    print(f\"  - {category}: {count} ({count/len(test_labels)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Recriação do Modelo\n",
    "\n",
    "Vamos recriar o modelo CNN e carregar os pesos treinados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir a arquitetura da CNN\n",
    "class CustomCNN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(CustomCNN, self).__init__()\n",
    "        \n",
    "        # Camadas convolucionais\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.conv4 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
    "        \n",
    "        # Camadas de pooling\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        # Camadas de batch normalization\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        self.bn4 = nn.BatchNorm2d(256)\n",
    "        \n",
    "        # Camadas fully connected\n",
    "        self.fc1 = nn.Linear(256 * 14 * 14, 512)\n",
    "        self.fc2 = nn.Linear(512, num_classes)\n",
    "        \n",
    "        # Dropout\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Bloco 1\n",
    "        x = self.pool(F.relu(self.bn1(self.conv1(x))))\n",
    "        \n",
    "        # Bloco 2\n",
    "        x = self.pool(F.relu(self.bn2(self.conv2(x))))\n",
    "        \n",
    "        # Bloco 3\n",
    "        x = self.pool(F.relu(self.bn3(self.conv3(x))))\n",
    "        \n",
    "        # Bloco 4\n",
    "        x = self.pool(F.relu(self.bn4(self.conv4(x))))\n",
    "        \n",
    "        # Flatten\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        # Fully connected\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Definir o dispositivo (GPU se disponível, senão CPU)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Usando dispositivo: {device}\")\n",
    "\n",
    "# Criar o modelo\n",
    "model = CustomCNN(num_classes=len(categories))\n",
    "\n",
    "# Carregar os pesos treinados\n",
    "if os.path.exists('../models/cnn/cnn_best.pt'):\n",
    "    model.load_state_dict(torch.load('../models/cnn/cnn_best.pt'))\n",
    "    print(\"Pesos treinados carregados com sucesso.\")\n",
    "else:\n",
    "    print(\"Pesos treinados não encontrados. Execute a Parte 2 primeiro.\")\n",
    "\n",
    "# Mover o modelo para o dispositivo\n",
    "model = model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Avaliação do Modelo no Conjunto de Teste\n",
    "\n",
    "Vamos avaliar o desempenho do modelo no conjunto de teste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para validar o modelo\n",
    "def validate(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(dataloader, desc=\"Avaliando\"):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Estatísticas\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    epoch_loss = running_loss / total\n",
    "    epoch_acc = correct / total\n",
    "    \n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "# Definir função de perda\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Avaliar no conjunto de teste\n",
    "test_loss, test_acc = validate(model, test_loader, criterion, device)\n",
    "print(f\"Teste - Perda: {test_loss:.4f}, Acurácia: {test_acc:.4f}\")\n",
    "\n",
    "# Calcular métricas detalhadas\n",
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        \n",
    "        y_true.extend(labels.cpu().numpy())\n",
    "        y_pred.extend(predicted.cpu().numpy())\n",
    "\n",
    "# Calcular métricas\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "precision = precision_score(y_true, y_pred, average='weighted')\n",
    "recall = recall_score(y_true, y_pred, average='weighted')\n",
    "f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "\n",
    "print(f\"Acurácia: {accuracy:.4f}\")\n",
    "print(f\"Precisão: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1-Score: {f1:.4f}\")\n",
    "\n",
    "# Matriz de confusão\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=categories, yticklabels=categories)\n",
    "plt.xlabel('Predito')\n",
    "plt.ylabel('Verdadeiro')\n",
    "plt.title('Matriz de Confusão')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Visualização de Predições\n",
    "\n",
    "Vamos visualizar algumas predições do modelo no conjunto de teste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para fazer predições em uma imagem\n",
    "def predict_image(model, image_path, transform, device, categories):\n",
    "    # Carregar a imagem\n",
    "    image = cv2.imread(image_path)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Aplicar transformações\n",
    "    image_tensor = transform(image).unsqueeze(0).to(device)\n",
    "    \n",
    "    # Medir o tempo de inferência\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Fazer a predição\n",
    "    with torch.no_grad():\n",
    "        outputs = model(image_tensor)\n",
    "        probabilities = F.softmax(outputs, dim=1)[0]\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "    \n",
    "    # Calcular o tempo de inferência\n",
    "    inference_time = time.time() - start_time\n",
    "    \n",
    "    # Obter a classe predita e a probabilidade\n",
    "    predicted_class = categories[predicted.item()]\n",
    "    probability = probabilities[predicted.item()].item()\n",
    "    \n",
    "    return image, predicted_class, probability, inference_time, probabilities.cpu().numpy()\n",
    "\n",
    "# Obter imagens de teste\n",
    "test_img_dir = '../dataset/test/images'\n",
    "test_img_files = [os.path.join(test_img_dir, f) for f in os.listdir(test_img_dir) if f.endswith(('.jpg', '.jpeg', '.png', '.avif'))]\n",
    "\n",
    "# Selecionar algumas imagens aleatórias\n",
    "random.seed(42)  # Para reprodutibilidade\n",
    "sample_imgs = random.sample(test_img_files, min(4, len(test_img_files)))\n",
    "\n",
    "# Fazer predições e visualizar\n",
    "plt.figure(figsize=(15, 12))\n",
    "inference_times = []\n",
    "\n",
    "for i, img_path in enumerate(sample_imgs):\n",
    "    # Fazer a predição\n",
    "    image, predicted_class, probability, inference_time, probabilities = predict_image(\n",
    "        model, img_path, test_transforms, device, categories\n",
    "    )\n",
    "    inference_times.append(inference_time)\n",
    "    \n",
    "    # Mostrar a imagem e a predição\n",
    "    plt.subplot(len(sample_imgs), 2, i*2+1)\n",
    "    plt.imshow(image)\n",
    "    plt.title(f\"Imagem: {os.path.basename(img_path)}\")\n",
    "    plt.axis('off')\n",
    "    \n",
    "    # Mostrar as probabilidades\n",
    "    plt.subplot(len(sample_imgs), 2, i*2+2)\n",
    "    plt.barh(categories, probabilities)\n",
    "    plt.title(f\"Predição: {predicted_class} ({probability:.2f}) - {inference_time:.3f}s\")\n",
    "    plt.xlim(0, 1)\n",
    "    plt.tight_layout()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calcular o tempo médio de inferência\n",
    "avg_inference_time = np.mean(inference_times)\n",
    "print(f\"Tempo médio de inferência: {avg_inference_time:.4f} segundos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Comparação com os Modelos YOLO\n",
    "\n",
    "Vamos comparar o desempenho da CNN com os modelos YOLO (customizado e tradicional)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar um DataFrame para comparação\n",
    "comparison_data = {\n",
    "    'Modelo': ['CNN do Zero', 'YOLO Customizado (30 épocas)', 'YOLO Customizado (60 épocas)', 'YOLO Tradicional'],\n",
    "    'Acurácia': [accuracy, 0.625, 0.625, 0.75],  # Valores obtidos dos resultados dos modelos\n",
    "    'Precisão': [precision, 0.40501, 0.89753, 0.48],\n",
    "    'Recall': [recall, 0.625, 0.625, 0.75],\n",
    "    'F1-Score': [f1, 0.54109, 0.76108, 0.60],\n",
    "    'Tempo de Inferência (s)': [avg_inference_time, 0.0030, 0.0008, 0.0736],\n",
    "    'Tempo de Treinamento (s)': [training_metrics['training_time'], 3600, 7200, 0]  # Estimativa para os modelos YOLO\n",
    "}\n",
    "\n",
    "# Criar o DataFrame\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "\n",
    "# Exibir o DataFrame\n",
    "display(comparison_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Análise Comparativa\n",
    "\n",
    "Vamos analisar e comparar os resultados dos diferentes modelos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparação de Desempenho\n",
    "\n",
    "Com base nos resultados obtidos, podemos comparar o desempenho da CNN treinada do zero com os modelos YOLO:\n",
    "\n",
    "1. **Precisão na Classificação vs. Detecção**:\n",
    "   - A CNN é um modelo de classificação, enquanto o YOLO é um modelo de detecção de objetos.\n",
    "   - A CNN classifica a imagem inteira, enquanto o YOLO detecta e classifica objetos específicos na imagem.\n",
    "   - Para tarefas de classificação simples, a CNN pode ser mais eficiente, enquanto para detecção de objetos em cenas complexas, o YOLO é mais adequado.\n",
    "\n",
    "2. **Tempo de Treinamento**:\n",
    "   - A CNN geralmente requer menos tempo de treinamento do que o YOLO, pois tem menos parâmetros e uma arquitetura mais simples.\n",
    "   - O YOLO customizado requer mais tempo de treinamento, especialmente com mais épocas.\n",
    "\n",
    "3. **Tempo de Inferência**:\n",
    "   - A CNN geralmente tem um tempo de inferência menor do que o YOLO, pois não precisa detectar objetos, apenas classificar a imagem inteira.\n",
    "   - O YOLO tradicional pode ser mais rápido que o YOLO customizado devido a otimizações específicas.\n",
    "\n",
    "4. **Facilidade de Uso**:\n",
    "   - A CNN é mais fácil de implementar e treinar do que o YOLO, pois tem uma arquitetura mais simples.\n",
    "   - O YOLO requer mais configuração e ajuste de hiperparâmetros.\n",
    "\n",
    "5. **Aplicabilidade**:\n",
    "   - A CNN é mais adequada para tarefas de classificação simples, onde a imagem contém apenas um objeto ou onde a classificação da imagem inteira é suficiente.\n",
    "   - O YOLO é mais adequado para tarefas de detecção de objetos em cenas complexas, onde é necessário localizar e classificar múltiplos objetos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusões\n",
    "\n",
    "1. **Quando usar a CNN**:\n",
    "   - Quando a tarefa é de classificação simples.\n",
    "   - Quando o tempo de treinamento e inferência são críticos.\n",
    "   - Quando os recursos computacionais são limitados.\n",
    "\n",
    "2. **Quando usar o YOLO customizado**:\n",
    "   - Quando a tarefa é de detecção de objetos específicos.\n",
    "   - Quando é necessário localizar e classificar múltiplos objetos em uma imagem.\n",
    "   - Quando a precisão na detecção é mais importante que o tempo de inferência.\n",
    "\n",
    "3. **Quando usar o YOLO tradicional**:\n",
    "   - Quando as categorias de interesse estão bem representadas no COCO.\n",
    "   - Quando não há tempo ou recursos para treinar um modelo customizado.\n",
    "   - Quando é necessário detectar uma variedade de objetos diferentes.\n",
    "\n",
    "Em resumo, a escolha entre CNN e YOLO depende da natureza da tarefa, dos recursos disponíveis e dos requisitos de desempenho. Para tarefas de classificação simples, a CNN pode ser mais eficiente, enquanto para detecção de objetos em cenas complexas, o YOLO é mais adequado."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
